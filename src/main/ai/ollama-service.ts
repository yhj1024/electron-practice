import type { OllamaConfig } from './types'

/**
 * í™˜ê²½ë³€ìˆ˜ì—ì„œ Ollama ì„¤ì • ë¡œë“œ
 * @throws {Error} í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ
 */
export function loadOllamaConfig(): OllamaConfig {
  const baseUrl = process.env.OLLAMA_BASE_URL
  const model = process.env.OLLAMA_MODEL
  const temperature = process.env.OLLAMA_TEMPERATURE

  // í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ê²€ì¦
  if (!baseUrl) {
    throw new Error('í™˜ê²½ë³€ìˆ˜ OLLAMA_BASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
  }

  if (!model) {
    throw new Error('í™˜ê²½ë³€ìˆ˜ OLLAMA_MODELì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
  }

  return {
    baseUrl,
    model,
    temperature: temperature ? parseFloat(temperature) : 0,
    streaming: false,
  }
}

/**
 * Ollama ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
 *
 * ì‹¤ì œ AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:
 * ```bash
 * pnpm add @langchain/ollama
 * ```
 *
 * ì‚¬ìš© ì˜ˆì‹œ:
 * ```typescript
 * import { ChatOllama } from '@langchain/ollama'
 *
 * const config = loadOllamaConfig()
 * const model = new ChatOllama(config)
 *
 * const response = await model.invoke('ì•ˆë…•í•˜ì„¸ìš”!')
 * console.log(response.content)
 * ```
 */
export class OllamaService {
  private config: OllamaConfig

  constructor() {
    this.config = loadOllamaConfig()
    console.log('ğŸ¤– Ollama ì„¤ì • ë¡œë“œ ì™„ë£Œ:', {
      baseUrl: this.config.baseUrl,
      model: this.config.model,
      temperature: this.config.temperature,
    })
  }

  /**
   * í˜„ì¬ Ollama ì„¤ì • ë°˜í™˜
   */
  getConfig(): OllamaConfig {
    return { ...this.config }
  }

  /**
   * AI ëª¨ë¸ê³¼ ì±„íŒ… (ìŠ¤íŠ¸ë¦¬ë°)
   * @param message ì‚¬ìš©ì ë©”ì‹œì§€
   * @param jobContext ì±„ìš©ê³µê³  ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
   * @param onChunk ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì½œë°±
   */
  async chatStream(
    message: string,
    jobContext: string | undefined,
    onChunk: (chunk: string) => void
  ): Promise<string> {
    try {
      const url = `${this.config.baseUrl}/api/generate`

      // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì»¨í…ìŠ¤íŠ¸
      let fullPrompt = message
      if (jobContext) {
        fullPrompt = `ë‹¹ì‹ ì€ ì±„ìš©ê³µê³  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒì€ ì±„ìš©ê³µê³  ì •ë³´ì…ë‹ˆë‹¤:

${jobContext}

ì‚¬ìš©ì ì§ˆë¬¸: ${message}

ìœ„ ì±„ìš©ê³µê³  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.`
      } else {
        fullPrompt = `ë‹¹ì‹ ì€ ì±„ìš©ê³µê³  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: ${message}`
      }

      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.config.model,
          prompt: fullPrompt,
          stream: true, // ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
          options: {
            temperature: this.config.temperature,
          },
        }),
      })

      if (!response.ok) {
        throw new Error(`Ollama API ìš”ì²­ ì‹¤íŒ¨: ${response.status} ${response.statusText}`)
      }

      // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
      let fullResponse = ''
      const reader = response.body?.getReader()
      const decoder = new TextDecoder()

      if (!reader) {
        throw new Error('ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      }

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        const chunk = decoder.decode(value, { stream: true })
        const lines = chunk.split('\n').filter(line => line.trim())

        for (const line of lines) {
          try {
            const json = JSON.parse(line)
            if (json.response) {
              fullResponse += json.response
              onChunk(json.response) // ì‹¤ì‹œê°„ìœ¼ë¡œ ì²­í¬ ì „ë‹¬
            }
          } catch (e) {
            // JSON íŒŒì‹± ì‹¤íŒ¨ ë¬´ì‹œ
          }
        }
      }

      return fullResponse || 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
    } catch (err) {
      console.error('AI ì±„íŒ… ì˜¤ë¥˜:', err)
      throw new Error(
        err instanceof Error ? err.message : 'AI ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
      )
    }
  }

  /**
   * AI ëª¨ë¸ê³¼ ì±„íŒ… (non-streaming, í•˜ìœ„ í˜¸í™˜)
   * @param message ì‚¬ìš©ì ë©”ì‹œì§€
   * @param jobContext ì±„ìš©ê³µê³  ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
   */
  async chat(message: string, jobContext?: string): Promise<string> {
    return this.chatStream(message, jobContext, () => {
      // ìŠ¤íŠ¸ë¦¬ë° ë¬´ì‹œ
    })
  }
}
